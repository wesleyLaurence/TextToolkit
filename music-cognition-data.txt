Music Cognition: Music’s Profound Effects on Brain Development, from Infancy through Adolescence
A Systematic Review

             Music is a ubiquitous and ancient phenomenon that has displayed significance in every human civilization in recorded history. It is a truly universal language that nearly every human inherently understands and connects with. Music has served as a fundamental aspect of the cultural, social, biological and cognitive evolution of mankind. Music is a triumph of human expression. Interest in the origins of music is an ancient fascination, leading to a great deal of theoretical research throughout thousands of years. With the development of modern science and brain imaging technology, researchers now possess the equipment necessary to empirically measure and deduce why music has such a profound impact on us. The increase in empirical, controlled experimental studies throughout the last twenty years has led to great strides in the quest to understand music.
      Throughout human development, music has major effects on the brain, mind, and overall health, acting as one of the most biologically enriching sensory experiences humans have access to. Music is a human universal, and serves as a social cohesive, synchronizing, uniting and strengthening societies and cultures through their native songs. Involvement with music also profoundly affects humans on an individual level, impacting many aspects of human cognition including communication, emotions, memory, learning, executive function, multimodal perception, motor coordination, and cognitive recovery throughout the life span.    
      In this systematic review, I will be discussing the myriad biological effects of musical involvement on human brain development from infancy, through adolescence. Musical involvement will be broadly defined as any activity that requires the perception of music. Various types of musical involvement will be discussed. The musical system primarily analyzed in this research is western tonal music. This paper utilizes research from the interdisciplinary field of music cognition. The follow fields of research were used in this review: neuroscience, psychology, musicology, ethnomusicology, music therapy, anthropology, sociology, linguistics, neurochemistry, neurobiology, functional medicine and kinesiology.
Key Concepts 
1. Music & Language; Auditory Perception 
2. Emotion, Attachment & Maternal Bonding
3. The Neurophysiology of Music in Infancy
4. Music & Language; Syntax 
5. Music & Reading
6. Rhythmic Entrainment, Attention & Executive Function
7. Emotions & Motivation
8. Music, Social Bonding & Expression
9. Creativity/Abstract Reasoning
I begin this report with an analysis of my compositions from a physiological perspective. The remainder focuses on music’s various effects on brain development.
Analysis of Compositions
      In my compositions, I used research from the fields of neurophysiology, neurochemistry and functional medicine to guide my compositional techniques. I will use information from empirical, observational studies of how music affects our nervous systems, to predict how my compositions will affect the physiology of the common, non-musician listener. I will not be using any human test subjects in this research. 
       Music is a cognitive experience, thus insight into how music affects our brains and nervous systems should be considered while composing. This research addresses various intervals, scales, rhythms and harmonies, and outlines their effects on human physiology. Through empirical studies, music scientists have gained substantial insight into how different types of music affects the human nervous system. A major theme throughout music cognition literature is the contrasting relationship between consonance and dissonance. Research shows that infants show a proclivity toward consonance, and an aversion toward dissonance (Peretz 2009). Ethnomusicologists have shown perfect fourths and fifths to be the most common throughout all musical cultures, with perfect fifths being the most common interval in world music (Patel 2008). In contrast, minor 2nd intervals and tritones have shown the potential to evoke negative physiological responses in listeners and thus are far less common in world music. Through a study done by Schellenberg and Trehub, it was shown that “perfect fourths and fifths are inherently easier to encode than are tritones” and the “inherent ease of processing consonant intervals has contributed to their prominence in most musical systems” (Peretz pg 7 [50]). Perfect fourths and fifths are not only easier for the brain to process, but also influence attention and emotion. Studies have shown that “infants are more attentive and exhibit more positive affect when listening to consonant music than to music with many dissonant intervals (Peretz 2009). Another study, shows that asymmetrical scales are inherently easier to process than symmetrical scales. This demonstrates an inherent predisposition for music with a tonal center, to act as a perceptual anchor point. The difficulty in processing symmetric scales evoked disorientation in common listeners, because of the “lack of a tonal center to provide a cognitive reference point for pitch perception” (Patel 2008). Regarding rhythm, it has been shown that infants inherently prefer patterns with regular rhythms rather than syncopated rhythms. This illuminates temporal processing predispositions for binary ratios. These various findings demonstrate that in general, world music capitalizes on human processing predispositions. Thus, the physiology of the auditory cortex and nervous system has contributed to the development of tonal systems in world music. 
             Our nervous systems are made up of two main systems; the central nervous system, and the peripheral nervous system. These systems are involved in the “maintenance of internal homeostatic processes, balancing approach and avoidance, excitation and inhibition, and fight or flight responses” (Cozolino p 52). The central nervous system is comprised of the brain and the spinal cord, and is our bodies primary regulatory system. The peripheral nervous system is made up of 2 sub-systems; the somatic and automatic systems. The somatic system controls movement and volition, while the automatic system regulates the body’s homeostasis as well as fight or flight. The automatic nervous system is the main emphasis of this analysis. This system is made up of the sympathetic nervous system and the parasympathetic nervous system. The sympathetic nervous system regulates the “secretion of hormones involved with the body’s response to stress and threat” (Cozolino p 52). This response accelerates heart rate, contracts blood vessels and raises blood pressure. This autonomic process is our fight or flight reflex, designed to prepare the body to either flee or fight for survival. The parasympathetic nervous system controls homeostasis in the body, and induces rest and restore. Heart rate slows, breathing slows, intestinal and glandular activity increases and muscles relax. Internal as well as external stimuli can trigger the activation of these different systems.
	The physical nature of different types of music affects our nervous systems in various ways. Consonant music has been shown to facilitate positive emotional responses, promoting the engagement of the parasympathetic nervous system. Several studies regarding music therapy on patients with arterial hypertension, showed that consonant music significantly decreased the level of cortisol (stress hormone) in the bloodstream, leading to notably reduced anxiety and pain in the patients (Mockel 1995). Consonant music has also been associated with increased levels of dopamine (pleasure hormone) in the bloodstream, promoting positive mood, and parasympathetic activation. Other factors such as moderate tempo, rhythm with binary ratios, and asymmetrical scales also typically contribute to positive affect and activation of the parasympathetic nervous system. In contrast, the physical nature of dissonant music has shown the potential to create stress hormones such as cortisol and adrenaline in the body, which can lead to the activation of the sympathetic nervous system. Various studies have shown dissonant music to raise blood pressure and increase cortisol production in non-musician listeners (Thoma 2013). A similar effect is found in music without a tonal center, containing syncopated rhythms. The sense of disorientation that occur in this style of music, can create stress in the common listener, resulting in negative affect and activation of the sympathetic nervous system. 
	In these compositions, I seek to facilitate sympathetic and parasympathetic responses from listeners through my compositional techniques. I base all compositional techniques on the empirical evidence gained from observational studies on how music effects our physiology.  
      In my first composition, “Sympathesia”, I sought to induce the sympathetic nervous system through dissonance, irregular rhythms and symmetrical scales. I attempt to induce this state by basing the composition off minor second, minor nine and tritone intervals. This piece is characterized by the dissonance found in the harmonic and melodic content. The harmony consists of several dissonant chords; major minor seventh, minor seven flat five and dominant 7 flat 9. This harmonic movement creates a stress inducing cacophony, designed to disturb the listener. I also utilized symmetrical scales, with the goal of disorienting the listener’s perception of a tonal center. Because people are physiologically inclined to asymmetrical scales, the use of the whole tone scale in this piece creates a sense of ambiguity and uncertainty. In another section of this composition, I transpose the main motif down a half step, while still maintaining the pitch of the original motif. This half step harmony creates a disorienting physiological response, and is likely to induce a cortisol response. I also use syncopated rhythms meant to disrupt the listener’s perception of beat. While the composition is in a 4/4 meter, the syncopated rhythms are designed to disrupt the temporal tracking within the listener. I also utilize sound design, sampling and heavy use of effects to attempt to illicit an associative fear response in the listener. The use of ambience in this piece is slightly morbid, and is influenced by growls, explosions and other fear associated sounds. The texture achieved in this piece utilizes a combination of heavily processed samples along with discordant sound design. Overall, I sought to create a sense of discomfort, uncertainty and fear. These feelings should trigger the sympathetic nervous system, causing increased heart rate, blood vessel contraction and raised blood pressure.
      With my second composition, “Dopaminia”, I sought to evoke a positive physiological response from the listener from the use of consonant intervals, moderate tempo, steady rhythms and a major mode. Through these compositional techniques, I created a pleasant, tonal piece, which is designed to evoke positive emotions in a common western listener. I placed a high priority on creating a harmonious sound texture through ambience, with the purpose of filling the frequency spectrum to create a lush and consonant atmosphere. The primary harmony is tonal, with smooth voice leading to promote consonance. The harmony instrumentation consists of a string quartet, piano, harp, and several synths. The melody is played on harp, and consists of simple combinations of consonant intervals. The moderate tempo in this piece is designed to rhythmically entrain the listener enough to evoke a dopamine response, without accelerating their heartrate. Overall, this piece makes use of consonance, moderate tempo and beat based rhythms to create a positive physiological and emotional response in the listener.
      My final composition, “Parasympathesia”, I composed a tonal piece of music designed to induce relaxation, by activating the parasympathetic nervous system. In this composition, I utilized perfect fourths and fifths to evoke a sense of consonance throughout the composition. The tempo is slow and the articulation is legato and smooth. The primary harmonic movement is between a Gadd9 and a Cadd9. The stacked fifths lay a lush foundation for the other harmonic and melodic content. The ambience of this piece is of high importance, designed to create a full and relaxing sound bed. I achieved this through the combination of natural ambience samples (rain, wind, waves, space) as well as synth design and pads. There is minimal dynamic contrast, to promote the listener to lose themselves in the atmosphere of the composition. Production techniques and use of effects processing was an essential component of creating the atmosphere for which the consonant composition resides. Overall, this composition should induce feelings of relaxation, activating the parasympathetic nervous system, resulting in a slower heart rate, deeper breathing, increased intestinal and glandular activity and muscle relaxation.
      

Infancy
- Auditory Perception; Music & Language
- Emotion, Attachment & Maternal Bonding
- The Neurophysiology of Music in Infancy
      In the first years of infancy, the young brain undergoes significant neural proliferation. Characterized by profound neuroplasticity, infant’s young brains are extremely malleable, allowing them to quickly and efficiently learn from and adapt to their surroundings. Throughout cognitive development, there are a series of critical periods which are defined as “limited periods in development when the effects of experience on the brain are unusually strong, derived from the property of particular malleability of the neural circuits” (Miendlarzewska 2014). These  “periods of exuberant neural growth” directly affect the structural and functional foundation of the brain (Cozolino 2006). In a combination of genetics and experience, the young brain constructs its framework for functionality and survival. 
     Auditory Perception: Music & Language
       In the first 12 months of life, the infant brain goes through a critical period for auditory perception. In this time frame, infants develop the fundamental neural mechanisms for processing music and language. This is an extremely sensitive time period in which drastic structural and functional changes occur in the brain, shaping the foundation of the auditory cortex, and connecting it to other brain regions. Auditory perception is shaped by two sound systems from an infant’s native culture; the (1) linguistic and the (2) musical. The linguistic system is shaped by the vowels, consonants, pitch contrasts and prosody of the native language. This system places high demands on the perception and categorization of timbre variation in the phenomes of the native language. The musical system is shaped by the pitch variation (intervals & chords), tone (duration, intensity, timbre) and rhythmic groupings of the native music. This system relies more heavily on the perception and categorization of pitch contrasts. So while linguistic and musical sound systems focus on different aspects of sound (timbre in language, pitch in music), the neural mechanisms used for the sound categorization have a substantial degree of overlap (Patel 2008). Thus, experience with instrumental music in infancy impacts language development, and vice versa. Here, we review the basis for musical and linguistic development, and discuss the underlying neural mechanisms involved in the two processes. 
      Linguistic sound systems are made up of two distinct perceptual aspects that characterize every native language. The first, phonetics is the acoustic, articulatory and auditory aspects of sound which allow language to be produced and perceived. The second, phonology is the study of the sound patterns of language and how speech sounds are organized into higher level units such as syllables, words, phrases and sentences which evoke semantic meaning. Through phonetics and phonology we are able to study the acoustic and articulatory parameters of sound as well as the organization of categorically defined elements. Language perception is very dependent on the sound categorization of different phenomes which are the minimal speech units that distinguish one word from another. All the different phenomes comprise the International Phonetic Alphabet, which is a standardized system for the representation of the sounds of spoken language. An important component of linguistic sound systems is that they are composed in a hierarchical structure, beginning with low level phenomes, then to syllables, then organized into words, phrases and sentences. This hierarchical organization is an essential element of linguistic systems, which enables simple phenomes to combine into complex propositional semantics. While variations in timbre help to characterize the distinctions between different words, contrasts in pitch and rhythm carry information about emotion, accent and phrasing. Thus, timbre helps illuminate the distinctions between semantic meaning of discrete words while pitch and rhythm provide an emotional and phrasal context, adding another dimension of meaning to language. In a broader context, aspects such as pitch, loudness, timing, accent and grouping of speech comprise the prosody of linguistic phrases to provide a richer context of meaning (Mannell, 2007). Because of the prosody of speech, humans are able to convey emotionally rich and meaningful concepts to people of our native culture. Through the perception and understanding of the hierarchical sound system of language, humans possess a powerful communicative ability that enables immense collaboration and progress. 
      Musical sound systems are comprised of a hierarchical organization of sound categories similar to that of speech. Musical systems are comprised of several characteristics of sound including pitch, rhythm, timbre, and amplitude. The aspect of sound given the most emphasis in musical systems is pitch. Pitch is defined as the “property of a sound that enables it to be ordered on a scale going from low to high” and is physically correlated to frequency (Patel 2008). Ain musical systems, pitches are organized into a pitch class or chroma, containing all notes within the octave. This is a fundamental, universal component of musical systems. Pitch classes are then further refined into musical scales, which are a “set of pitches and intervals within the octave that serve as reference points in the creation of musical patterns” (Patel 2008). Scales are a universal concept in music and are extremely diverse when examined cross culturally. Descending in the hierarchy, scales are comprised of smaller units called intervals. Scales are made up of different combinations of intervals within any given octave. These intervals are the basis for musical scales, motives, phrases, and compositions. And lastly, we have single pitches, which combine to create intervals. This hierarchical organization of pitches is a fundamental concept for musical systems in nearly every culture. Timbre also plays a major role in the aesthetic perception of musical systems. Although in musical systems timbre is not organized into discrete sound categories like in speech, musical timbre creates diversity in the sound quality of different instruments, synthesizers and voices. This aesthetic component enables widespread diversity in the music of different cultures. Rhythm also plays an essential role in musical sound systems, enabling sounds to be “organized into patterns in terms of timing, accent and grouping” (Patel 96). Every native culture’s music is comprised of these different sound categories, and the hierarchical structure of musical systems enables diverse and rich communication.
      To analyze musical systems from a cognitive perspective, intervals are encoded in the brain as learned sound categories, much like the timbral patterns of speech. The native music of any given culture directly impacts the cognitive organization and auditory processing of pitch relations in the brain. These neural mechanisms and learned sound categories are what enable our coherence with music and the relative information it presents. 
      There is a substantial amount of similarity in the neural mechanisms used for learning the sound categories of music and language. Within the first year of life, infants are hard at work, taking statistics on their native music and language, and developing their categorical perception. This allows them to differentiate the distinct variations between timbre in language and pitch of music. Infants are able to distinguish small changes in musical scales, melodies and rhythms, indicating fundamental, possibly genetic, inherent biological mechanisms apparent in very early life. Experiments also show that in the first year of life, infants are able to discriminate certain speech contrasts from foreign languages better than adults. This ability diminishes after the first year of life. This presents the idea that the infant brain possesses a powerful statistical learning mechanism for sound categorization, allowing them to learn native language and music in the first year of life with astounding efficiency. During this critical period for auditory perception, infants create the neural foundations for processing music and language. When infants are exposed to the sound systems of their culture, the same brain mechanisms are organizing both musical and linguistic sounds into discrete categories. The neural overlap between the processing of music and language demonstrates fundamental similarities in the sound categorization mechanisms of music and language. Thus, musical exposure in infancy contributes to a stronger ability to learn the sound categories of language. This has practical applications in the study of human communicative development and suggests that phonological abilities can be enhanced by exposure to instrumental music in infancy.  
       

Emotion, Attachment & Maternal Bonding
             Infants are brought into the world with a genetic predisposition to create emotional attachments with their primary caregivers. This biologically inherent instinct serves to organize the infant’s brain in preparation for the many social attachments they will experience in life. These early relationships shape the neural foundations for the infant’s implicit, internal model of attachment. This is a very vulnerable stage of life, and the caregiver has an essential responsibility to nurture the infant, in order for them to develop a healthy sense of attachment. Their sense of security and implicit social tendencies are reliant on these first several attachment relationships in infancy. The infant’s first and most profound relationship in life is with their mother, where they first establish physical contact, nourishment and security. Music therapy and maternal singing have been shown to lead to improved infant health, as well as stronger emotional attachment between the mother and the infant. 
             The mother-infant relationship is characterized by emotional communication. “Emotion is at the heart of attachment” and therefore plays a primary role in the social development of infants (Siegel 80). When infants hear their mother’s voice, they are primarily extracting the emotional tone and musicality of the speech, rather than the literal meaning. Infants are unable to understand the semantic meaning of language until childhood; thus, the emotional tone of their caregiver’s voice is of utmost importance. Emotional tone is largely influenced by pitch contour, which is a major feature in both speech and music. Infants have been shown to demonstrate heightened sensitivity to pitch contour (Peretz 5). Thus, maternal singing with pitch variation creates an emotional tone that leads to increased attachment between the mother and infant. Other nonverbal cues such as “facial expression, eye gaze, tone of voice, bodily gestures, and the timing and intensity of response each are also fundamental to emotional messages” (Siegel 80). In the first year of life, an infant’s right cortical hemisphere (used for processing non-verbal & emotional information) is much larger and more dominant than the left hemisphere. Not until childhood does the left cortical hemisphere (used for verbal information, semantic language, logic) catch up with the right, as the child learns the basics of their native language (Patel 2008). Thus, emotional connection is vital to communication between the mother and infant, and this first relationship sets the foundation for the infant’s implicit sense of attachment throughout their life. 
             Involving music in an infant’s early maternal attachment has shown to have positive neurophysiological and neurochemical effects, as well as create stronger emotional bonds between infants and mothers. Consonant music has therapeutic neurophysiological effects on the mother, which releases a cascade of positive endorphins and hormones, helping to create a healthy neurochemical environment within the mother. The neurochemical state of the mother is very important in yielding positive attachment between the infant and mother. Stress hormones such as adrenaline and cortisol have shown to have negative effects on maternal attachment. Music has been shown to release the endorphins oxytocin, serotonin and dopamine, which facilitate trust and emotional attachment between the infant and mother. In these early years, infants achieve their emotional state through a “self-regulation that is actually determined in part by an interactive “dyadic” process of mutual co-regulation” (Siegel 81). Thus, the emotional state and mental processes of the mother directly impacts the emotional state of the infant. Mothers engaged in music therapy with their infants have shown to produce cascades of positive endorphins, resulting in a nurturing, enriching neurochemical state. This results in a positive, stress free environment for development. Secondly, maternal singing to an infant has shown to greatly increase the emotional attachment between the mother and infant. Because of the affective nature of music, maternal singing increases emotional communication between the mother and infant leading to enhanced attachment and more efficient regulation of arousal. One study shows the results of maternal singing after stress induction and showed that the singing was very effective in “decreasing infant’s physiological arousal, regulating negative emotion and promoting infants’ visual attention” (Ghazban 2016). Thus, maternal singing is an efficient tool for regulating stress and creating stronger emotional attachments between mothers and infants.
             Music has positive neurophysiological, neurochemical and emotional effects on infant health as well as mother-infant attachment. Music therapy and maternal singing are excellent ways to strengthen emotional attachment and physically enrich the health of an infant.  


The Neurophysiology of Music in Infancy
             Exposure to music in infancy has been shown to have numerous positive neurophysiological effects. It has been observed that music has the potential to regulate stress, promote positive affect and improve cognitive recovery in infancy. Music regulates our heartrate, impacts our neurochemical environment and facilitates the creation of new cells after cognitive degeneration or stress. Through controlled, experimental studies, we are beginning to gain insight into the physical properties of sound and music that contribute to its positive effect on our physiology. For example, it has been shown that humans have a biological predisposition to be positively impacted by consonant intervals such as perfect fourths and fifths, as opposed to more dissonant intervals such as the tritone and minor second. In one experimental study, four-month-old infants demonstrated preference for Twinkle, Twinkle played in major and minor thirds as opposed to the same melody with all minor second intervals. This stems from the physicality of the frequencies involved with these different intervals and how they physiologically effect the auditory cortex and the nervous system. Thus, exposure to consonant music can have numerous positive effects on an infant’s physiology. This has implications for musical intervention in infancy and can help guide therapeutic and clinical efforts.
            Through several experimental studies, consonant music has shown to be successful in the regulation of stress. The stress response results from interference or a threat to the homeostasis of a living organism. This results in the triggering of the sympathetic nervous system. With the sympathetic nervous system triggered, the brain releases hormones such as cortisol and epinephrine (adrenaline) into the blood stream to prepare the organism to fight for survival. As the myriad hormones flood our body, our heart rate increases, sending more blood to muscles and vital organs and our breathing rate rises to increase oxygenation. These physiological changes help to ensure highest survival rate in times of danger. While the stress response is evolutionarily designed as a protective agent, the results of high levels of chronic stress can result in numerous negative impact on our health. Chronic stress has been shown to increase the possibility of depression, anxiety, digestive problems, headaches and memory impairment. From a clinical perspective, extraneous stress on a human brain should be highly avoided to ensure proper development. One experiment analyzed the effects of consonant music on premature babies when getting a foot prick (acquiring blood from the foot of an infant by needle). After the initial stress of the skin penetration, data showed the experimental group who received the musical treatment had significantly lower heartrates, indicating a reduction in the stress response. This has significant and practical implications in the regulation of stress in infants.
            Music has also commonly demonstrated its positive effects on emotions and the limbic system. Pleasurable music has been shown to produce a cascade of endorphins such as dopamine, serotonin and oxytocin, resulting in positive emotions and improved mood. Production of dopamine also plays a part in facilitating learning, improving attention and flushing stress hormones out of the bloodstream. 


Childhood
- Music & Language; Syntax 
- Music & Reading
- Rhythmic Entrainment, Attention & Executive Function 
      As infancy wanes, childhood emerges and brain development shifts gears to begin to accommodate higher level processes. This time is characterized by several critical periods of growth, resulting in significant development of the left cortical hemisphere, peak synaptic proliferation and increased integration throughout the brain (Bianchi 2013). With many of the fundamental neural structures for basic bottom up processing already developed, the child has the perceptual capabilities necessary to begin learning more complex behaviors such as verbal communication, reading, and executive function.
Music & Language; Syntax
      Throughout childhood, kids are presented with the concept of syntax, which is defined as “the arrangement of words and phrases to create well-formed sentences in a language” (Webster). Both linguistic and musical systems are highly syntactic, involving the “combining of discrete structural elements into sequences” (Patel 240). While the principles of combination differ between music and linguistic syntax, the two processes actually share several fundamental elements. Based on the similarities between linguistic and musical syntax, it has been shown empirically that there is significant neural overlap in syntactic processing in the two domains. Here, we will review the basis for musical and linguistic syntax, discuss the similarities between the two domains, and consider the similarities in the neural processing of musical and linguistic syntax.
      Linguistic syntax is a rich, hierarchical system involving many layers. This is a very versatile system which can be manipulated in myriad ways which enables complex communication. In language, the most basic subunit of meaning, morphemes, combine to create words, which combine to create phrases, which combine to form sentences. In linguistic systems there is a unique connection between syntax and meaning. Merely changing the syntax of a sentence can entirely change the meaning of a sentence (i.e. The man with the thin cane saw the girl; The thin girl with the cane saw the man) (Patel 245). This contingent relationship between linguistic syntax and semantic meaning is a characteristic feature of language. Another rich aspect of linguistic systems is that “words can take on abstract grammatical functions (such as subject, direct object, and indirect object) that are determined by their context and structural relations rather than by inherent properties of the words themselves” (Patel 244). Words and their absolute meaning can take on new meanings when applied in different relative contexts. In a broader context, linguistic syntax provides a set of principles in which to combine discrete elements into sequences which convey semantic meaning. Language is a rich, hierarchical system which integrates multiple levels of syntax to express explicit semantic meaning. 
      Like language, musical systems are rich, hierarchical systems that integrate multiple layers of syntactic features. Musical syntax of tonal music can be broken down into several levels of pitch organization. In tonal music, tones combine to create musical scales, which combine to form chords and harmony, which combine to form chord progressions within a given key. This multilayered structure is dynamic and allows for many possible variations within the parameters. It is important to note that while the syntactic principles of language create a contingent relationship between syntax and semantic meaning, musical syntax differs, with its lack of explicit, semantic meaning. Thus, the principles of musical syntax are much less explicit than linguistic syntax. However, empirical evidence shows that within musical scales, the different scale degrees possess variant amounts of stability in relation to the tonic chord. While the IV and V scale degrees are quite stable, the II and the VII are relatively unstable and seek resolution. One study by Robert Francis (1988) provided evidence for the “pull to the tonic” showing our inherent proclivity to seek resolution in tonal music. Chords are organized horizontally in time to create sequences and progressions. The horizontal chordal context plays a major role in our perception of tonal music. Chords are combined in series to imply tension and resolution which comprises the larger form of a musical structure. The sequences of chords within a given piece of music contribute to its “pattern of tonal tension that arises from relations between harmonic elements within a structured cognitive space” (Patel 257). These principles of syntactic organization aid in the creation of melody, harmony and rhythm in western tonal music. 
      Because of the complexities and similarities of both linguistic and musical syntax, pursuit of a unifying hypothesis to compare the two domains has been greatly sought after. Although linguistic and musical syntactic systems possess different principles of sequencing, the processes underlying the neural organization and integration have shown to have a substantial degree of overlap. The primary goal of this research is elucidating the neural overlap between the processing of linguistic and musical syntax. One hypothesis, the “shared syntactic integration resource hypothesis” (SSIRH) states that the “overlap in the syntactic processing of language and music can be conceived as overlap in the neural areas and operations that provide the resources for difficult syntactic integration” (Patel 283). In other words, there is a significant degree of overlap between the neural systems used to integrate musical and linguistic syntax. There have been several empirical studies done to support this subject. One study tested the idea that when performing certain tasks which involve both musical and linguistic syntax, there would be interaction and compensation between the two processes. They sought to show that integrating linguistic syntax while simultaneously integrating musical syntax would cause an observable interference. They did in fact find interference, proving that there is a shared network of neural resources for musical and linguistic syntactic processes. Another study, looked at the influence of harmonic manipulations on syntactic integration. The results showed that “processing a harmonically unexpected chord interfered with the processing of syntactic but not semantic, relations in language” (Patel 2008). Thus, the syntactic anomaly an out of key chord, led to syntactic errors in language processing. This research demonstrates that the integration of musical and linguistic syntax share common neural resources in the brain. 
      These findings have practical implications for cognitive development and education. With music and language utilizing highly similar neural systems, it is plausible that training in one domain would have overlap effects on the other.
Music & Reading
      As children are learning to speak, they are also becoming familiar with the written language of their native culture. While written language is similar to spoken language, from a cognitive perspective, there are some important distinctions. Learning to read requires distinct brain networks which enable the perception of complex visual patterns, and the integration of syntactic and semantic meaning derived from symbols. Recent empirical studies on music training’s impact on reading abilities have brought to light some interesting connections between the two domains. Observational studies are now gathering evidence that rhythmic musical training increases reading scores on standardized tests. Based on a comprehensive study conducted by Adam Tierney and Nina Kraus, there are five subskills required in reading which are known to be impacted by musical training; (1) phonological awareness (2) speech-in-noise perception (3) rhythm perception (4) auditory working memory and (5) the ability to learn sound patterns. The neural synchrony hypothesis states that reading abilities are derived from a common neural foundation for temporal and frequency resolution, rapid auditory processing and phonological awareness. This implies that Through empirical experiments in the last few years, it has been shown that musical training improves auditory processing, having crossover effects on reading abilities. Given the intimate relations between language and music processing, “musical training might provide an effective developmental educational strategy for all children, including those with language learning impairments” (Kraus 2014). 
	As stated before, there are five main subskills required for reading that have been shown to improve with musical training. One of the most fundamental prerequisites for learning to read is (1) phonological awareness. This is the “explicit knowledge of the components of speech and how they can be combined” (Kraus 2014). Awareness of phenomes, and the ability to combine different syllables is an essential ability that is necessary for pronunciation in reading. Phonological awareness is a skill characterized by the ability to perceive, categorize and differentiate speech sounds. This requires accurate temporal encoding of acute changes in timing frequency and timbre. Because the differences between phenomes can be very minimal (e.g, ta and da), reading requires intense auditory discrimination between slightly different sounds. Musical training has been shown to increase children’s ability to learn sound categories and discriminate between sounds with miniscule variations in timing and frequency. Musicians have demonstrated faster auditory processing of music and speech sounds, more accurate encoding of frequency information and better differentiation between speech sounds. 
	Another important component of reading abilities is (2) perception of speech sounds in noise. When background noise disrupts our perception of speech, it places higher demands on the auditory cortex. Children with auditory perception deficits have an especially hard time with speech and reading when background noise is present because it disrupts the auditory representation of the phenomes, and diminishes the ability to properly differentiate speech sounds. Empirical evidence shows that involvement with music “increases the auditory system’s resilience to noise and other sources of signal degradation, decreasing the effects of background noise and reverberation on response amplitude, timing, and encoding of speech harmonics (Kraus 2014). This demonstrates that musical training could possibly benefit children with auditory perception deficits, and increase their chances of learning to speak, listen and read. 
	Empirical evidence has recently demonstrated the importance of (3) rhythmic competence in the ability to segment speech and maintain phonological awareness. The ability to track rhythmic patterns within a given beat has shown to assist in the durational awareness of sounds in speech. The Dynamic Attending Theory proposes “a set of neural oscillators that phase-lock and resonate to the temporal structure of music, resulting in an attentional focus that waxes and wanes, following the rhythmic structure of a piece or song” (Kraus 2014). Evidence of this has been shown through experiments that demonstrate peoples enhanced ability to perform perceptual tasks when they are synchronized to a beat. A similar mechanism has been proposed to explain how we track speech amplitude over time. This explanation, the Temporal Sampling Hypothesis “proposes that delta/theta oscillatory phase-locking selectively “samples” the low-frequency information in the amplitude envelope, which is crucial for the segmentation of speech and discrimination of speech sounds” (Kraus 2014). Evidence suggests that these mechanisms could be regulated by a central brain function that regulates both rhythm in music and envelope in speech. Deficits in this ability could be signs of decreased auditory neural synchrony. Several studies back up these findings by showing a relationship between children’s ability to discriminate between different rhythms in beat based music and reading ability. The main finding is that rhythmic training and the ability to synchronize to a beat has been shown to increase reading abilities in children.  
	Another aspect of reading is the use of (4) auditory working memory. This refers to the ability to maintain conscious attention of auditory stimulus for a short period of time. Being able to “keep an auditory sequence in mind long enough to decode it into its component sounds” is a important aspect of speech and reading. With a deficit in this area, a child might have a hard to holding onto what they hear long enough to decode its meaning, leading to the inability to understand long phrases. Almost all aspects of musical perception rely on auditory working memory. Performance and memorization of musical pieces’ places high demands on auditory memory and attention. Attentive music listening also exercises the auditory working memory, making it easier for musicians to track long linguistic phrases in speech and reading.
	The final aspect of reading is the ability to (5) learn sound patterns. To become sufficient in language, one must be possess the phonological awareness to distinguish one phenome from another. This is rooted in the ability to perceive, categorize and memorize miniscule variations in sound. There are regularities among the different sounds of language that make it easier to understand. These structural regularities allow for the more efficient categorization and recognition of phenomes in language tracking. Music presents an interesting comparison given the amount of structural regularities within beat based music. Research shows that children involved with music were more successful at tracking regularities and segmentations in speech. This ability to extract meaningful patterns from auditory stimuli contributes to musician’s superior ability to learn to read. 
	The recent increase in empirical research in music cognition literature regarding music, language and reading presents some very practical implications. Through theoretical and empirical studies it is shown “that musical training can lead to increased neural synchrony throughout the auditory system, suggesting that music could be an effective way to boost reading skills in children” (Kraus 2014). 

Rhythmic Entrainment, Attention & Executive Function
 	Of the many rich components that comprise musical experience, one of special interest throughout music cognition literature is rhythm. Rhythm of western music can be defined as the “underlying temporal pattern that is called meter, which defines a hierarchical structure between time points” (Miendlarzewska 2014). This temporal organization is unique to music and is fundamental for our ability to perceive a beat and synchronize our attention and movements accordingly. The concept of rhythmic entrainment suggests that “musical activities that imply perception and production of rhythms train attentional processes which benefits also other cognitive functions” (Miendlarzewska 2014). Thus, the main area of exploration in this article is how musical training and rhythmic entrainment assists in the formation of attention and executive function in the brain. Executive function is comprised of “four discrete but inter-related executive domains; (1) attentional control, (2) cognitive flexibility, (3) goal setting, and (4) information processing” (Anderson 2010). Studies have demonstrated the crossover effects of rhythmic training on other cognitive functions such as speech, reading, motor coordination, sensorimotor integration and attention. Throughout childhood, there are several notable critical periods for the development of attentional networks, which are fundamental aspects of cognitive integration. Musical training and rhythmic entrainment in childhood fosters the development of executive function and aids in the integration of cortical brain structures. 	
      The development of executive functions begins in early infancy, and continues through early adolescence. By the middle of childhood, the brain has achieved attentional control (attention & inhibition), which allows a child to selectively decide what they pay attention to. This is the first step to executive function development. As childhood unfolds, cognitive flexibility (task switching), goal setting and information processing go through rapid development, including a critical period between the age of seven and nine (Miendlarzewska 2014). It is important to note that the critical period for executive function is an extremely vital time in cognitive development, and ultimately dictates the structural layout of attentional networks in the brain. Therefore, the structural development and integration of executive function networks will result in individual’s ability to control attention, inhibit inappropriate behavior, regulate emotion, plan and reason abstractly. A major theme of executive function development is the concept of integration. Integration refers to the wiring together of various brain regions and their corresponding mental functions, resulting in a singular executive attention (working memory). Cognitive integration connects the cerebrum, (temporal, parietal, occipital lobes), cerebellum and brainstem to the frontal lobes (anterior cingulate cortex & prefrontal cortex). This allows the prefrontal cortex (aka CEO of the brain), to make informed decisions, utilizing information from all brain regions (Bell 2014). This results in the integration of top-down and bottom-up processing, which leads to better cognitive, emotional integration. Top down information consists of “higher-order mental functions such as attention, imagery, expectations and learned associations” whereas bottom-up information is “supplied by computations that are inherent in the circuitry of our brains” and can be considered hard wired, native abilities (Kandel 22). This is a fundamental aspect of cognitive maturation, ultimately leading to profound neural connectivity, enabling very complex and abstract thoughts and movements. 
      Musical training and rhythmic entrainment have shown to have various effects on the development of executive function in children. Studies have shown musical training’s beneficial crossover effects on mental abilities such as cognitive flexibility, working memory capacity and motor coordination. One study which tested rhythmic abilities, demonstrated that the “ability to tap to a beat was associated with better performance not only in reading but also in other attention-demanding tasks which are purportedly at the basis of executive functions” (Miendlarzewska 2014). These crossover effects reveal that by engaging with beat based temporal music, a synchronization of attention processes occurs, facilitating the integration of numerous brain areas involved with executive function. Several studies showed that “tapping to, producing or merely perceiving a rhythm in any sensory domain leads to formation of expectations that facilitates orienting of attentional resources and entrainment of various bodily and neural functions” (Miendlarzewska 2014). Various related studies have led to the theory that musical training increases the integration of top down and bottom up processing in children. By increasing the integration between top-down and bottom-up processing, executive functions can be enhanced, leading to a better ability to orient attention, plan and reason. 
      This research demonstrates musical training’s powerful ability to shape the development of human executive functions during childhood. Rhythmic entrainment promotes the synchronization and orientation of attentional systems to temporal, beat based patterns, which results in the strengthening of executive functions as well as increased integration throughout the brain. Thus, musical training has the potential to improve cognitive development and wire the brain for success in the future.  
      

Adolescence 
- Emotions & Motivation
- Music, Social Bonding & Expression
- Creativity/Abstract Reasoning
   Adolescence is a complex and very significant time in brain development. This time is characterized by profound hormonal and physical changes, along with a shift in identity, self-consciousness and cognitive flexibility. Profound development in physical, behavioral and social maturity unfold in a relative synchrony of various pubertal adaptations. The drastic functional and structural changes in the brain at this time, enables more self-reflective awareness as well as an ability to understand multidimensional concepts. As “processing speed of neural information in the frontal cortex increases” adolescents can engage in high level thinking such as empathy, abstract reasoning and creativity (Blakemore 2003). Because of the drastic reorganization of the frontal cortex during adolescence, young people are exceptionally vulnerable to making decisions based on impulse, rather than rational, planned thought. This notion has led to the idea that adolescents are careless risk takers by their own volition, however their risky behavior might just be a side effect of the rapid brain organization in the frontal lobes. (Blakemore 2003). This is an essential time of human growth, and research will continue to illuminate the underpinnings of adolescence. 
Emotions & Motivation
      As children transition from childhood to adolescence, a series of radical physiological shifts occur, characterized by significant physical, cognitive and emotional maturation. Following a series of neural shifts in late childhood, the brain produces a cascade of hormones which marks the onset of puberty. Adolescents undergo increased physical growth, sexual maturity, and a shift in social dynamics. Some of the notable changes induced by puberty are “romantic and sexual interests, mood lability, emotional intensity, reward/sensation seeking, changes in sleep/arousal regulation, increased appetite and risk for affective disorders” (Dahl 2004). The simultaneous maturation of many different frontal lobe systems in adolescence results in sporadic behavior, increased risk-taking and emotional instability. This period typically involves experimentation with various hobbies, sports, sciences and arts. Research has demonstrated that music provides numerous benefits on adolescent brain development, impacting emotional and motivational systems. From a cognitive perspective, adolescents experience a flood of strong hormonal emotions, without yet having the proper inhibitory brain systems to reason, consider possible consequences and regulate emotions. Thus, adolescence is a critical period for the development of emotional regulation. During this complex time of growth, music serves as a powerful tool to foster positive emotional development throughout adolescence, and develop neural foundations that can have lasting benefits well into adulthood.
      A large aspect of brain development in adolescence involves the integration of cognitive and emotional functions. Adolescents must learn emotional regulation (controlling one’s feelings and modulating them to achieve goals and support decision making). This is an essential component of adolescent development; mastery of “deliberate and conscious application of top-down executive control over processing of an emotional stimulus” (Etkin 2010). As the adolescent brain becomes flooded with hormones, they experience notably intense emotional experiences. This increase in hormones and emotions can result in noticeable behavior changes in adolescents, who can come across as careless and even recalcitrant. Thus, a vital part of adolescence is developing the ability to regulate these emotions. Research shows that music is a powerful tool for fostering positive emotional regulation in adolescents. 
      From a cognitive perspective, music has been shown to activate emotional regions of the brain such as the ventral striatum (NAc), medial prefrontal cortex (MPFC) and anterior cingulate cortex (ACC) (Janata 2009). The NAc is commonly known to regulate “motivation and reward-related experiences of pleasure” (Koelsch 2010). The MPFC supports the “integration of memories, emotions and music” and “associating music and memories when we experience emotionally salient episodic memories that are triggered by familiar songs from our personal past” (Janata 2009). This underlies music’s ability to trigger vivid, autobiographical, emotional memories. The ACC has shown to be an integral brain region for emotional regulation. The NAc MPFC and ACC collaborate with other brain regions to regulate the pleasure, emotions and memories associated with music. Passive music listening releases the neurotransmitters dopamine, serotonin and oxytocin, resulting in a pleasurable feeling that play a key role in learning, motivation and volition. 
      This regulatory effect that music has on emotions is a viable way to regulate the emotional state of a volatile adolescent. Thus, after experiencing a negative emotional experience, music serves as an excellent way to restore a positive neurochemical environment that supports cognitive recovery. One randomized, controlled study, demonstrated the effects of music therapy on adolescents with depression. Adolescents who received music therapy showed “ improved self-esteem and significantly reduced depression” as well as “improved communicative and interactive skills” (MNT 2016). This has very practical implications for cognitive development and appears to be a viable method for increasing emotional regulation abilities as well as treating depression in adolescence. 
      Along with general emotional regulation, music has shown to have a profound effect on motivation. Motivation is defined as the process that initiates, guides and regulates goal oriented behavior. As adolescent’s mature and branch out into the social world, motivation plays a major factor in their development of identity and personal goals. Young people are now cognitively capable of imagining themselves in the future in either a positive or negative way. These early neural patterns form the basis of their motivational system and can impact the way they see themselves and the world throughout adulthood. Research has demonstrated music’s profound impact on motivation and the reward system through studies in neurophysiology and neurochemistry. Insight into the dopaminergic transmission system elucidates music’s ability to regulate motivation. From a musical perspective, dopamine, which serves many functions in the limbic system, appears to activate the NAc, while enriching it with dopamine which serves as a motivational agent to guide willful and goal oriented behavior (Koelsch 2010).  In other words, the simple act of listening to pleasurable music can stimulate motivation based behavior, resulting in more willful behavior. With adolescence being a very sensitive period for emotional and motivational development, musical intervention in adolescence can have numerous positive impacts, leading to well-integrated and healthy emotional regulation. 
      Music has demonstrated numerous positive effects on emotional regulation and motivation. Music therapy has shown to be effective in counteracting depression as well as improving the mental state of healthy adolescents. There are many practical implications for this research and further studies will continue to elucidate the profound effects of music on adolescents. 


Music; Social Bonding & Expression
      Music has played an essential role in the neurobiological, cognitive and social evolution of mankind. Music making and movement (dance) are “activities central to ritual, courtship, identity, and human expression cross-culturally” (Tarr 2014). Music is a core human experience; a true universal . As Timothy Blanning noted, music is “a grand ‘triumph’ of the human condition, spanning across cultures to reach the greatest of heights in the pantheon of human expression, communication, and well-being” (Schulkin 2014).  Music and spoken language coevolved, leading to a proliferation and diversification in human’s communicative abilities. Excavated musical instruments dating back 40,000 years, suggesting that music is a fundamental constituent in the long term evolution of homo sapiens. Music enabled humans to express complex, emotionally laden messages, leading to increased connectedness and social strength (Schulkin 2014). Historically, rhythm based music has played a “significant role both in creating social bonds and indicating coalition strength” (Tarr 2014). Rhythmic music is a unique phenomenon, which increases human connectedness through rhythmic entrainment, social synchrony, and “self-other merging”. Music has an implicit effect on our motor coordination, with evidence of “synchronized exertive movements” having a major impact on our sense of connectedness. From a developmental perspective, the social nature of music has many beneficial effects on cognitive development, especially during adolescence. Music contributes to the fostering of many aspects of social cognition development including identity, emotional expression, empathy and social connectedness. Here we will review the various elements of music’s impact on social cognition and discuss the known effects it evokes on adolescent brain development. 
      Music is an ancient form of communication. Some historians believe that humans might have sang before learning to speak in syntactically guided sentences. Musical communication enabled humans to express emotionally laden messages, facilitating stronger emotional connections among tribes, and thus strengthening human socialization. Throughout evolution, music has continued to diversify, allowing for increasingly complex emotional communication (Schulkin 2014). It is notable to say that empirical research on music’s effects on social cognition is limited, leading to more focus on theoretical research. As adolescents go through puberty, their prefrontal cortex goes through considerable development, resulting in more self-awareness, perspective taking and emotional communication. From a developmental perspective, exposure to music in adolescence appears to have beneficial effects on emotional communication, however the lack of thorough experimental studies limits the discussion on this topic. There have been a few studies in this regard. Involvement with music has been shown to promote prosocial behavior and increased cooperation in groups. One study demonstrated that joint singing/drumming increased cooperation among the participants, leading to increased communication and understanding within the group. Other studies have shown that passive music listening, free from motor engagement, within a group leads to increased feelings of connectedness. Thus, the mere act of listening to music with others, appears to strengthen emotional connectedness and increases communication within the group (Schulkin 2014). This is a fruitful field of research, however empirical work in this topic is in it’s infancy. Theoretical research on this topic suggests that music can have profound effects on adolescent emotional communication, however more research is required. To fully understand music’s effect on emotional communication, researchers must continue to gather objective data through controlled experiments. 
      A major theme throughout music and social cognition literature is the concept of synchronization. The temporal organization found in rhythmic, beat based music is characterized by the “externalization of predictable rhythms that allow synchronization to occur between two or more people” (Tarr 2014). It is suggested that group rhythmic entrainment to a beat, facilitates the modulation of the endogenous opioid system, resulting in a cascade of endorphins that promote social connectedness. Synchronized group exertions have been shown to release oxytocin, which results in “increased trust, eye contact, face memory, generosity, empathy and the ability to infer the mental state of others”, however the connection between music and oxytocin is still tenuous (Tarr 2014). However, pleasure from music listening as well as synchronized group exertion appear to be mediated by endorphins, thus a connection very well might underlie their relationship, however more empirical work must be done to verify. From a developmental perspective, rhythmic entrainment and synchronization appear to have very notable benefits on brain development. Rhythmic entrainment has shown to have effects on prefrontal cortex development, and appear to have effects on social connectedness as well. 
      Empirical research on adolescent brain development is still in it’s infancy. It is important to note that many of the connections between music and social cognition development are speculative, however it appears that music greatly impacts social development from infancy all the way through adolescence. This is a very fruitful area of research for cognitive scientists, and further research will continue to reveal the profound effects of music on the adolescent brain.    


Creativity/Abstract Reasoning
      Artistic expression and creativity are among the most significant cultural achievements of human evolution. Our species uniquely powerful cognitive, emotional and social abilities allow us to express our inner cognitive and emotional state, through an external medium and share it with others. The continuous acculturation of human society has led to a rich and diverse array of emotional and aesthetic articulation. Conceptual creativity is the driver of human progress, enabling humans to create new tools with which to drive growth and innovation. Through scientific methods, researchers are attempting to identify the possible connections between musical creativity and other forms of creativity. While still in its empirical infancy, creativity has accumulated many theoretical studies, attempting to elucidate the neural mechanisms involved, and clarify how they relate to other brain functions. 
       Researchers are seeking to understand where musical creativity originates from a cognitive perspective, how it effects other mental processes and if creativity in music has crossover effects on forms of non-musical creativity such as divergent thinking and combinational creativity. Divergent thinking is a thought process used to generate creative ideas by exploring many possible solutions. Combinational creativity involves the “generation of unfamiliar combinations of familiar ideas” (Boden 2013). It is important to note that creativity is a tricky concept, from a cognitive perspective, and there is very much left to be discovered in this domain. However, there are various theories of creativity that are quite plausible, but need to be tested empirically to validate. One theory from the Neuroscience of Creativity (2013) suggests that there are several types of creativity. The style of creativity that appears to be the most related to musical creativity is combinational creativity. This type of creativity appears to be very relatable to musical creativity, given that creating music involves the generation of unfamiliar combinations of notes and rhythms, derived from the same pitches and rhythms used throughout world music. Boden discusses the relevance of “conceptual hierarchies” in combinational creativity, and how through association and relevance, we are able to generate novel concepts. Given the importance of hierarchies in the organization of musical systems, it is plausible to deduce that musical creativity may rely on a similar style of combinational creativity; understanding the hierarchical structure of music, and using those parts to generate novel combinations of melody, harmony and rhythm. 
       Along with a theoretical approach, we will discuss creativity from an empirical, neuroscientific perspective, through the analysis of fMRI and PET scans of jazz musicians during improvisation. These brain scans will then be compared to brain scans of subjects performing divergent thinking. Through analysis of a series of related studies, this research seeks to produce a new hypothesis, proposing that the neural resources and psychological processes which mediate musical creativity are the same resources and processes that mediate divergent thinking and combinational creativity.
      From a theoretical perspective, the processes underlying musical creativity appear to be similar to those of combinational creativity. The generation of novel combinations within a given conceptual hierarchy is the essence of musical creativity. Through an understanding of the structural, hierarchical organization of music (pitches, intervals, scales, harmony, rhythm), musicians are able to assemble those parts into new combinations, yielding creativity. While the content of musical hierarchies is distinctly different from that of linguistic or other conceptual hierarchies, the psychological process of putting together novel combinations within a given hierarchy appears to be similar throughout all forms of creativity. Research in the syntax of music and language sheds some interesting light on this topic. The concept of syntax will be broadly defined as “principles of combination that govern the structure of sequences”. Creativity appears to have an inherent relationship with syntax. Through novel syntactic combinations, creativity emerges, yielding novel ideas. Theoretically, the relationship between musical and linguistic syntax, and combinational creativity appears to fundamental. Thus, it is plausible to deduce that musical, linguistic and conceptual creativity are embodied within combinational creativity. This suggests that these various forms of creativity are cognitively and psychologically related.  
      An excellent model for empirically studying musical creativity is through the analysis of jazz improvisation. The spontaneous creativity that characterizes improvisation is a cognitively complex process that researchers are seeking to elucidate through studies in neuroanatomy and neuroscience. Recent studies in jazz improvisation brings light to some interesting findings involving the functional and structural neuroanatomy of jazz musicians. In a study done by Charles J. Limb, researchers used fMRI to look at jazz musicians’ brains while they performed an improvised solo. They discovered that during musical improvisation there was a “dissociated pattern of activity in the prefrontal cortex: extensive deactivation of dorsolateral prefrontal and lateral orbital regions with focal activation of the medial prefrontal cortex” (Limb 2008). These findings suggest a pattern of neural activity in which the cognitive mechanisms for self-monitoring, inhibition and self-critiquing were deactivated. This implies a unique mode of thought, where information can flow through the unconscious brain, and into the conscious mind without experiencing the strong inhibitory effect of the dorsolateral prefrontal region, enabling “creative self-expression in the absence of conscious self-monitoring” (Limb, 2008). This absence of inhibition appears to be a fundamental component of musical creativity.  
      Another creative mode of thought is divergent thinking (a thought process used to generate creative ideas by exploring many possible solutions). This is also a quite cognitively complex behavior which requires integration of many different brain regions into working memory. There have been several studies investigating the neuroanatomy of divergent thinking, attempting to identify the neural mechanisms involved. In one study by Dongtao Wei, they utilized fMRI and observed “resting-state functional connectivity (RSFC), which reflects temporal correlations between blood oxygen level-dependent signals in different brain regions during rest” (Wei 2013). The participants were evaluated for their divergent thinking abilities by a standardized test called the Torrance Tests of Creative Thinking (TTCT). The researchers were investigating possible correlation between the “strength of RSFC between the medial prefrontal cortex (mPFC) and the middle temporal gyrus (mTG). In the divergent thinkers, they found a strong relationship between these two distinct brain regions. The medial prefrontal cortex (mPFC) and the middle temporal gyrus (mTG) showed high RSFC, suggesting that these brain regions are connected to divergent thinking. Thus, musical creativity and divergent thinking appear to rely on the medial prefrontal cortex, for generation of novel combinations.
      Are the cognitive mechanisms used for musical creativity, combinational creativity, and divergent thinking related? While the informational hierarchies of music, language and other concepts have distinct differences, empirical research and neuroimaging shows an overlap in the neural mechanisms used to mediate the processes. As the research showed, the medial prefrontal cortex was focally activated in both musical creativity and divergent thinking, suggesting that the two functions are cognitively similar. Thus, integration from the medial cortex appears to mediate musical creativity, combinational creativity and divergent thinking. These findings suggest that musical creativity training could have positive overlapping effects on combinational creativity and divergent thinking.   
      
      
Conclusion
      Music is an ancient and universal human activity and has played a major role in the evolution of humanity. On the surface, music communicates emotions, is the basis for dance, ties social groups together and creates joy in individuals, but it’s underlying cognitive processes are diverse and complex. Theoretical and empirical research has begun to shed light on music’s profound impact on brain development. Producing significant effects on linguistic communication, physiology, emotions, social bonding, attention and creativity, music continues to be the most cognitively and emotionally enriching activity humans possess. Music has strong physiological effects, which regulates our health and well-being throughout the entire lifespan. Music serves as is a primary aspect of our autobiographical memory, giving us identity and meaning throughout our entire lives. Music is a triumph of the human condition and will continue to enrich the lives of humans for generations to come.  


Appendix 
Artificial Intelligence
      Recent advances in computer science have led to profound transformations in almost every industry. Automation has led to proliferating dynamism and the emergence of new bustling industries. With recent developments in machine learning, high performance computing and artificial intelligence (AI), computer systems are now capable of performing functions that emulate human thought. Through deep neural networks, computer systems can now take in colossal amounts of data, and through complex algorithms, extract underlying patterns and make decisions based on statistical probability and objective data. Deep neural networks have yielded wonderful features such as recommendation systems based on personal interests, computer vision, voice recognition, and even artistic creativity. The immense power of these deep neural networks are “cognifying” our utility systems and making them smarter at an exponential rate. I propose that utilizing deep neural networks to personalize music education would have profound effects on the learning experience and give educators objective insight into the learning process. An AI music education system would provide a personalized learning experience for each unique individual, while providing complete visibility for teachers into learning progress. 
      Computer science and artificial intelligence will continue to revolutionize our world. Let the age of bots begin. :)  